<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta name="description"
        content="ILDiff">
    <meta name="keywords" content="Text-to-Video, Diffusion Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>xiaoyuan1996.github.io</title>

    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
        </script>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>
</head>

<body>


<section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
<!--        <div class="columns is-centered">-->
          <div class="column has-text-left">
            <h3 class="title is-3 publication-title">
                Zhiqiang Yuan
            </h3>
            <h5 class="title is-5 publication-title">
<!--                <img class="center" width="100" height="35" src="assets/RDTF.png">-->
                Location: Beijing, China
            </h5>
            <h5 class="title is-5 publication-title">
<!--                <img class="center" width="100" height="35" src="assets/RDTF.png">-->
                Work: Wechat AI, Tencent
            </h5>
            <h5 class="title is-5 publication-title">
<!--                <img class="center" width="100" height="35" src="assets/RDTF.png">-->
                Email: yuanzhiqiang19@mails.ucas.ac.cn
            </h5>
            <h5 class="title is-5 publication-title">
<!--                <img class="center" width="100" height="35" src="assets/RDTF.png">-->
                 <a href="https://scholar.google.com.hk/citations?user=gtfpYs8AAAAJ&hl=zh-CN" style="font-size:13pt;" >GoogleScholar</a>
                 <a href="https://github.com/xiaoyuan1996" style="font-size:13pt;" >Github</a>
            </h5>
            <div class="is-size-5 publication-authors">

              </div>
  
            </div>
          </div>
<!--        </div>-->
      </div>
    </div>
  </section>
  


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-left">
            <p>
            Graduated with a bachelor's degree from Harbin Engineering University in 2019, and obtained a doctorate in engineering from Aerospace Information Research Institute, Chinese Academy of Sciences in 2024.
            From 2024 to present, working at WeChat AI, long-term committed to the research and application of deep learning methods under sample and resource-constrained conditions.
            </p>
      </div>


    <section class="section" id="Research">
        <h3 class="title is-3">Research</h3>

        <hr />
        <h4 class="title is-4">Deep Learning Under Limited Samples</h4>

        <li class="title is-5">  Remote Sensing Cross-modal Retrieval Method Based on Limited Samples </li>

            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * Remote sensing cross-modal text-image retrieval based on global and local information

                <br>
                <span style="color: white;">space *</span>
                [TGRS 2022, ESI Highly Cited, first author]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:IjCSPb-OGe4C" style="font-size:13pt;" >[paper]</a>
                <a href="https://github.com/xiaoyuan1996/GaLR" style="font-size:13pt;" >[mainpage]</a>

            </h4>

            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * A lightweight multi-scale crossmodal text-image retrieval method in remote sensing

                <br>
                <span style="color: white;">space *</span>
                [TGRS 2021, first author]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:qjMakFHDy7sC" style="font-size:13pt;" >[paper]</a>
                <a href="https://github.com/xiaoyuan1996/retrievalSystem" style="font-size:13pt;" >[mainpage]</a>

            </h4>

            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * MCRN: A multi-source cross-modal retrieval network for remote sensing

                <br>
                <span style="color: white;">space *</span>
                [JAG 2022, first author]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:WF5omc3nYNoC" style="font-size:13pt;" >[paper]</a>
                <a href="https://github.com/xiaoyuan1996/MCRN" style="font-size:13pt;" >[mainpage]</a>

            </h4>

                    <h4 class="title is-6">

                <span style="color: white;">space </span>
                * Learning to Evaluate Performance of Multi-modal Semantic Localization

                <br>
                <span style="color: white;">space *</span>
                [TGRS 2022, first author]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:YsMSGLbcyi4C" style="font-size:13pt;" >[paper]</a>
                <a href="https://github.com/xiaoyuan1996/SemanticLocalizationMetrics" style="font-size:13pt;" >[mainpage]</a>

            </h4>


            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * SeLo v2: Toward for higher and faster semantic localization

                <br>
                <span style="color: white;">space *</span>
                [GRSL 2023, corresponding author]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:Zph67rFs4hoC" style="font-size:13pt;" >[paper]</a>
                <a href="https://github.com/xiaoyuan1996/SemanticLocalizationMetrics/blob/master/README_v2.md" style="font-size:13pt;" >[mainpage]</a>

            </h4>





        <li class="title is-5">  Model Robustness Design </li>

            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * Visual transformer with stable prior and patch-level attention for single image dehazing

                <br>
                <span style="color: white;">space *</span>
                [Neurocomputing 2023, corresponding author]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:ULOm3_A8WrAC" style="font-size:13pt;" >[paper]</a>
            </h4>

            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * Frequency compensated diffusion model for real-scene dehazing

                <br>
                <span style="color: white;">space *</span>
                [NeuralNetworks 2024, first author as a student]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:YOwf2qJgpHMC" style="font-size:13pt;" >[paper]</a>
                <a href="https://github.com/W-Jilly/frequency-compensated-diffusion-model-pytorch" style="font-size:13pt;" >[mainpage]</a>                
            </h4>

            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * Diffusion Model with Detail Complement for Super-Resolution of Remote Sensing

                <br>
                <span style="color: white;">space *</span>
                [RemoteSensing 2022, corresponding student]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:eQOLeE2rZwMC" style="font-size:13pt;" >[paper]</a>
            </h4>


            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * A Synthetic-to-Real Dehazing Method based on Domain Unification

                <br>
                <span style="color: white;">space *</span>
                [ICME 2025, first author]
                <!-- <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:eQOLeE2rZwMC" style="font-size:13pt;" >[paper]</a> -->
            </h4>

        <li class="title is-5">  Dataset Construction </li>
            <h4 class="title is-6">

                <span style="color: white;">space </span>
                * Exploring a fine-grained multiscale method for cross-modal remote sensing image retrieval

                <br>
                <span style="color: white;">space *</span>
                [TGRS 2021, ESI Highly Cited, first student]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:2osOgNQ5qMEC" style="font-size:13pt;" >[paper]</a>
                <a href="https://github.com/xiaoyuan1996/AMFMN" style="font-size:13pt;" >[mainpage]</a>
            </h4>

           <h4 class="title is-6">

                <span style="color: white;">space </span>
                * VSD2M: A Large-scale Vision-language Sticker Dataset for Multi-frame Animated Sticker Generation

                <br>
                <span style="color: white;">space *</span>
                [ICME 2025, first author]
                <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:iH-uZ7U-co4C" style="font-size:13pt;" >[paper]</a>
                <a href="./files/VSD2M/index.html" style="font-size:13pt;" >[mainpage]</a>
            </h4>



        <hr />
        <h4 class="title is-4">Deep Learning Under Limited Resources</h4>

        <li class="title is-5">Design of Distillation Method</li>

        <h4 class="title is-6">

            <span style="color: white;">space </span>
            * ILDiff: Generate Transparent Animated Stickers by Implicit Layout Distillation

            <br>
            <span style="color: white;">space *</span>
            [ICASSP 2025, oral, first author]
            <a href="https://ieeexplore.ieee.org/abstract/document/10888420" style="font-size:13pt;" >[paper]</a>
            <a href="./files/ILDIFF/index.html" style="font-size:13pt;" >[mainpage]</a>

        </h4>


        <h4 class="title is-6">

            <span style="color: white;">space </span>
            * Efficient and Controllable Remote Sensing Fake Sample Generation Based on Diffusion Model

            <br>
            <span style="color: white;">space *</span>
            [TGRS 2023, first author]
            <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:0EnyYjriUFMC" style="font-size:13pt;" >[paper]</a>
            <a href="https://github.com/xiaoyuan1996/Controllable-Fake-Sample-Generation-for-RS" style="font-size:13pt;" >[mainpage]</a>

        </h4>

        <li class="title is-5"> Lightweight Model Design and Application </li>



        <h4 class="title is-6">

            <span style="color: white;">space </span>
            * WalkVLM: Aid Visually Impaired People Walking by Vision Language Model

            <br>
            <span style="color: white;">space *</span>
            [ICCV 2025, first author]
            <a href="https://scholar.google.com.hk/citations?view_op=view_citation&hl=zh-CN&user=gtfpYs8AAAAJ&citation_for_view=gtfpYs8AAAAJ:TFP_iSt0sucC" style="font-size:13pt;" >[paper]</a>
            <a href="./files/walkvlm/index.html" style="font-size:13pt;" >[mainpage]</a>


        </h4>

    </section>


</body>

